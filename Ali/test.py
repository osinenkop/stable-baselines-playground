import matplotlib
import matplotlib.pyplot as plt
import gymnasium as gym
import argparse
import numpy as np
import time

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from gymnasium.wrappers import TimeLimit
from mygym.my_pendulum import PendulumRenderFix
# Import the custom callback from callback.py
from callback.plotting_callback import PlottingCallback
from stable_baselines3.common.utils import get_linear_fn
from controller.pid import PIDController
from controller.energybased import EnergyBasedController

# Initialize the argument parser
parser = argparse.ArgumentParser(description="PPO Training and Evaluation for Pendulum")
parser.add_argument("--notrain", action="store_true", help="Skip the training phase")

# Parse the arguments
args = parser.parse_args()

matplotlib.use("TkAgg")  # Try "Qt5Agg" if "TkAgg" doesn't work

# Register the environment
gym.envs.registration.register(
    id="PendulumRenderFix-v0",
    entry_point="mygym.my_pendulum:PendulumRenderFix",
)

# Use your custom environment for training
env = gym.make("PendulumRenderFix-v0")

env = TimeLimit(env, max_episode_steps=500)  # Set a maximum number of steps per episode

controller = EnergyBasedController()

# Load the model (if needed)
model_PPO = PPO.load("ppo_pendulum")
model_calf = PPO.load("ppo_pendulum_calf")

result_controller = []
result_PPO = []
result_Calf = []

dt = 0.05

print("Test Start")
for i in range(30):
# Reset the environment
    obs, _ = env.reset()
    cos_theta, sin_theta, angular_velocity = obs
    acc_rewards = []
    print("Test Controller")
    # Run the simulation and render it
    for _ in range(500):
        # Compute the control action using the nominal controller
        control_action = controller.compute(cos_theta, angular_velocity)
        # Clip the action to the valid range for the Pendulum environment
        action = np.clip([control_action], -2.0, 2.0)
        obs, reward, done, _, _ = env.step(action)
        acc_rewards.append(reward)
        # Update the observation
        cos_theta, sin_theta, angular_velocity = obs

    result_controller.append(np.mean(acc_rewards)) 

    obs, _ = env.reset()
    cos_theta, sin_theta, angular_velocity = obs
    done = False
    acc_rewards = []
    print("Test PPO")
    for _ in range(500):
    # Compute the control action using the nominal controller
        # Action generated by the agent
        action, _ = model_PPO.predict(obs)

        obs, reward, done, Truncate, _ = env.step(action)
        acc_rewards.append(reward)

    result_PPO.append(np.mean(acc_rewards))

    obs, _ = env.reset()
    cos_theta, sin_theta, angular_velocity = obs
    done = False
    acc_rewards = []
    print("Test CaLF")
    for _ in range(500):
    # Compute the control action using the nominal controller
        # Action generated by the agent
        action, _ = model_calf.predict(obs)
        obs, reward, done, Truncate, _ = env.step(action)
        acc_rewards.append(reward)

    result_Calf.append(np.mean(acc_rewards))

    print(i)

print(f"Total average Controller = {np.mean(result_controller)}")
print(f"Total average PPO = {np.mean(result_PPO)}")
print(f"Total average CaLF = {np.mean(result_Calf)}")